{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "from LLTMpython.LLTM import LLTM as LLTMpython\n",
    "from LLTMcpp.LLTM import LLTM as LLTMcpp\n",
    "from LLTMcuda.LLTM import LLTM as LLTMcuda\n",
    "from LLTMfast.LLTM import LLTM as LLTMfast\n",
    "from LLTMfastseq.LLTM import LLTM as LLTMfastseq\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "cuda_device = torch.device(\"cuda\")  # device object representing GPU\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "input_features = 300\n",
    "state_size = 300\n",
    "input_seq_len = 2000\n",
    "\n",
    "X = torch.randn(batch_size, input_seq_len, input_features, device=cpu_device)\n",
    "h = torch.randn(batch_size, state_size, device=cpu_device)\n",
    "C = torch.randn(batch_size, state_size, device=cpu_device)\n",
    "\n",
    "# Note the device=cuda_device arguments here\n",
    "X_gpu = torch.randn(batch_size, input_seq_len, input_features, device=cuda_device)\n",
    "h_gpu = torch.randn(batch_size, state_size, device=cuda_device)\n",
    "C_gpu = torch.randn(batch_size, state_size, device=cuda_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm python on cpu: Forward: 109.580 us | Backward 120.619 us\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# test basic python lltm on cpu\n",
    "\n",
    "lltm_python = LLTMpython(input_features, state_size).to(cpu_device)\n",
    "\n",
    "python_cpu_forward = 0\n",
    "python_cpu_backward = 0\n",
    "\n",
    "xx = X\n",
    "hh = h\n",
    "cc = C\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):\n",
    "    new_hh, new_cc = lltm_python(xx[:,t,:].squeeze(), (hh, cc))\n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "python_cpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "python_cpu_backward += time.time() - start\n",
    "\n",
    "print('lltm python on cpu: Forward: {:.3f} us | Backward {:.3f} us'.format(python_cpu_forward * 1e6/input_seq_len, python_cpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm python on gpu: Forward: 149.078 us | Backward 143.149 us\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# test basic python lltm on gpu\n",
    "\n",
    "lltm_python = LLTMpython(input_features, state_size).to(cuda_device)\n",
    "\n",
    "python_gpu_forward = 0\n",
    "python_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu\n",
    "hh = h_gpu\n",
    "cc = C_gpu\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):\n",
    "    new_hh, new_cc = lltm_python(xx[:,t,:].squeeze(), (hh, cc))    \n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "torch.cuda.synchronize()\n",
    "python_gpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "torch.cuda.synchronize()\n",
    "python_gpu_backward += time.time() - start\n",
    "\n",
    "print('lltm python on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(python_gpu_forward * 1e6/input_seq_len, python_gpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm cpp on cpu: Forward: 90.843 us | Backward 193.920 us\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# test cpp extended lltm on cpu\n",
    "\n",
    "lltm_cpp = LLTMcpp(input_features, state_size).to(cpu_device)\n",
    "\n",
    "cpp_cpu_forward = 0\n",
    "cpp_cpu_backward = 0\n",
    "\n",
    "xx = X\n",
    "hh = h\n",
    "cc = C\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):\n",
    "    new_hh, new_cc = lltm_cpp(xx[:,t,:].squeeze(), (hh, cc))\n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "cpp_cpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "cpp_cpu_backward += time.time() - start\n",
    "\n",
    "print('lltm cpp on cpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cpp_cpu_forward * 1e6/input_seq_len, cpp_cpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm cpp on gpu: Forward: 117.098 us | Backward 315.422 us\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# test cpp extended lltm on gpu\n",
    "\n",
    "lltm_cpp = LLTMcpp(input_features, state_size).to(cuda_device)\n",
    "\n",
    "cpp_gpu_forward = 0\n",
    "cpp_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu\n",
    "hh = h_gpu\n",
    "cc = C_gpu\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):\n",
    "    new_hh, new_cc = lltm_cpp(xx[:,t,:].squeeze(), (hh, cc))    \n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "torch.cuda.synchronize()\n",
    "cpp_gpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "torch.cuda.synchronize()\n",
    "cpp_gpu_backward += time.time() - start\n",
    "\n",
    "print('lltm cpp on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cpp_gpu_forward * 1e6/input_seq_len, cpp_gpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm cuda on gpu: Forward: 98.993 us | Backward 95.511 us\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# test cuda lltm on gpu\n",
    "\n",
    "lltm_cuda = LLTMcuda(input_features, state_size).to(cuda_device)\n",
    "\n",
    "cuda_gpu_forward = 0\n",
    "cuda_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu\n",
    "hh = h_gpu\n",
    "cc = C_gpu\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):\n",
    "    new_hh, new_cc = lltm_cuda(xx[:,t,:].squeeze().contiguous(), (hh, cc))    \n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "torch.cuda.synchronize()\n",
    "cuda_gpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "torch.cuda.synchronize()\n",
    "cuda_gpu_backward += time.time() - start\n",
    "\n",
    "print('lltm cuda on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cuda_gpu_forward * 1e6/input_seq_len, cuda_gpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm fast cuda on gpu: Forward: 391.358 us | Backward 93.962 us\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# test fast cuda lltm on gpu\n",
    "\n",
    "lltm_fast = LLTMfast(input_features, state_size).to(cuda_device)\n",
    "\n",
    "fast_gpu_forward = 0\n",
    "fast_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu\n",
    "hh = h_gpu\n",
    "cc = C_gpu\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):  \n",
    "    new_hh, new_cc = lltm_fast(xx[:,t,:].squeeze().contiguous(), (hh, cc))    \n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "torch.cuda.synchronize()\n",
    "fast_gpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "torch.cuda.synchronize()\n",
    "fast_gpu_backward += time.time() - start\n",
    "\n",
    "print('lltm fast cuda on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(fast_gpu_forward * 1e6/input_seq_len, fast_gpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm fastseq cuda on gpu: Forward: 659.819 us \n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# test fastseq cuda lltm on gpu\n",
    "\n",
    "lltm_fastseq = LLTMfastseq(input_features, state_size).to(cuda_device)\n",
    "\n",
    "fastseq_gpu_forward = 0\n",
    "fastseq_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu\n",
    "hh = h_gpu\n",
    "cc = C_gpu\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "new_hh, new_cc = lltm_fastseq(xx, (hh, cc))    \n",
    "torch.cuda.synchronize()\n",
    "fastseq_gpu_forward += time.time() - start\n",
    "\n",
    "\n",
    "\n",
    "print('lltm fastseq cuda on gpu: Forward: {:.3f} us '.format(fastseq_gpu_forward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm on gpu: Forward: 26.439 us \n",
      "gru on gpu: Forward: 19.110 us \n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# test torch lstm on gpu\n",
    "\n",
    "lstm = torch.nn.LSTM(input_features, state_size, bias=True).to(cuda_device)\n",
    "\n",
    "lstm_gpu_forward = 0\n",
    "lstm_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu.transpose(0,1)\n",
    "hh = h_gpu.unsqueeze(0)\n",
    "cc = C_gpu.unsqueeze(0)\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "new_xx, (new_hh, new_cc) = lstm(xx, (hh, cc))    \n",
    "torch.cuda.synchronize()\n",
    "lstm_gpu_forward += time.time() - start\n",
    "\n",
    "\n",
    "\n",
    "print('lstm on gpu: Forward: {:.3f} us '.format(lstm_gpu_forward * 1e6/input_seq_len))\n",
    "\n",
    "# ========================================\n",
    "# test torch gru on gpu\n",
    "\n",
    "gru = torch.nn.GRU(input_features, state_size, bias=True).to(cuda_device)\n",
    "\n",
    "gru_gpu_forward = 0\n",
    "gru_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu.transpose(0,1)\n",
    "hh = h_gpu.unsqueeze(0)\n",
    "cc = C_gpu.unsqueeze(0)\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "new_hh, new_cc = gru(xx, hh)  \n",
    "torch.cuda.synchronize()\n",
    "gru_gpu_forward += time.time() - start\n",
    "\n",
    "\n",
    "\n",
    "print('gru on gpu: Forward: {:.3f} us '.format(gru_gpu_forward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm python on cpu: Forward: 109.580 us | Backward 120.619 us\n",
      "lltm python on gpu: Forward: 149.078 us | Backward 143.149 us\n",
      "lltm cpp on cpu: Forward: 90.843 us | Backward 193.920 us\n",
      "lltm cpp on gpu: Forward: 117.098 us | Backward 315.422 us\n",
      "lltm cuda on gpu: Forward: 96.472 us | Backward 96.521 us\n",
      "lltm fastseq cuda on gpu: Forward: 60.485 us \n"
     ]
    }
   ],
   "source": [
    "# print all results\n",
    "print('lltm python on cpu: Forward: {:.3f} us | Backward {:.3f} us'.format(python_cpu_forward * 1e6/input_seq_len, python_cpu_backward * 1e6/input_seq_len))\n",
    "print('lltm python on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(python_gpu_forward * 1e6/input_seq_len, python_gpu_backward * 1e6/input_seq_len))\n",
    "print('lltm cpp on cpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cpp_cpu_forward * 1e6/input_seq_len, cpp_cpu_backward * 1e6/input_seq_len))\n",
    "print('lltm cpp on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cpp_gpu_forward * 1e6/input_seq_len, cpp_gpu_backward * 1e6/input_seq_len))\n",
    "print('lltm cuda on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cuda_gpu_forward * 1e6/input_seq_len, cuda_gpu_backward * 1e6/input_seq_len))\n",
    "#print('lltm fast cuda on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(fast_gpu_forward * 1e6/input_seq_len, fast_gpu_backward * 1e6/input_seq_len))\n",
    "print('lltm fastseq cuda on gpu: Forward: {:.3f} us '.format(fastseq_gpu_forward * 1e6/input_seq_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 16, 300])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
