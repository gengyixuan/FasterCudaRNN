{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "from LLTMpython.LLTM import LLTM as LLTMpython\n",
    "from LLTMcpp.LLTM import LLTM as LLTMcpp\n",
    "from LLTMcuda.LLTM import LLTM as LLTMcuda\n",
    "from LLTMfast.LLTM import LLTM as LLTMfast\n",
    "from LLTMfastseq.LLTM import LLTM as LLTMfastseq\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "cuda_device = torch.device(\"cuda\")  # device object representing GPU\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "input_features = 300\n",
    "state_size = 64\n",
    "input_seq_len = 2000\n",
    "\n",
    "X = torch.randn(batch_size, input_seq_len, input_features, device=cpu_device)\n",
    "h = torch.randn(batch_size, state_size, device=cpu_device)\n",
    "C = torch.randn(batch_size, state_size, device=cpu_device)\n",
    "\n",
    "# Note the device=cuda_device arguments here\n",
    "X_gpu = torch.randn(batch_size, input_seq_len, input_features, device=cuda_device)\n",
    "h_gpu = torch.randn(batch_size, state_size, device=cuda_device)\n",
    "C_gpu = torch.randn(batch_size, state_size, device=cuda_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm python on cpu: Forward: 138.787 us | Backward 144.174 us\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# test basic python lltm on cpu\n",
    "\n",
    "lltm_python = LLTMpython(input_features, state_size).to(cpu_device)\n",
    "\n",
    "python_cpu_forward = 0\n",
    "python_cpu_backward = 0\n",
    "\n",
    "xx = X\n",
    "hh = h\n",
    "cc = C\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):\n",
    "    new_hh, new_cc = lltm_python(xx[:,t,:].squeeze(), (hh, cc))\n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "python_cpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "python_cpu_backward += time.time() - start\n",
    "\n",
    "print('lltm python on cpu: Forward: {:.3f} us | Backward {:.3f} us'.format(python_cpu_forward * 1e6/input_seq_len, python_cpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm python on gpu: Forward: 145.556 us | Backward 149.473 us\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# test basic python lltm on gpu\n",
    "\n",
    "lltm_python = LLTMpython(input_features, state_size).to(cuda_device)\n",
    "\n",
    "python_gpu_forward = 0\n",
    "python_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu\n",
    "hh = h_gpu\n",
    "cc = C_gpu\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):\n",
    "    new_hh, new_cc = lltm_python(xx[:,t,:].squeeze(), (hh, cc))    \n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "torch.cuda.synchronize()\n",
    "python_gpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "torch.cuda.synchronize()\n",
    "python_gpu_backward += time.time() - start\n",
    "\n",
    "print('lltm python on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(python_gpu_forward * 1e6/input_seq_len, python_gpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm cpp on cpu: Forward: 125.665 us | Backward 222.874 us\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# test cpp extended lltm on cpu\n",
    "\n",
    "lltm_cpp = LLTMcpp(input_features, state_size).to(cpu_device)\n",
    "\n",
    "cpp_cpu_forward = 0\n",
    "cpp_cpu_backward = 0\n",
    "\n",
    "xx = X\n",
    "hh = h\n",
    "cc = C\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):\n",
    "    new_hh, new_cc = lltm_cpp(xx[:,t,:].squeeze(), (hh, cc))\n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "cpp_cpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "cpp_cpu_backward += time.time() - start\n",
    "\n",
    "print('lltm cpp on cpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cpp_cpu_forward * 1e6/input_seq_len, cpp_cpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm cpp on gpu: Forward: 117.089 us | Backward 316.190 us\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# test cpp extended lltm on gpu\n",
    "\n",
    "lltm_cpp = LLTMcpp(input_features, state_size).to(cuda_device)\n",
    "\n",
    "cpp_gpu_forward = 0\n",
    "cpp_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu\n",
    "hh = h_gpu\n",
    "cc = C_gpu\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):\n",
    "    new_hh, new_cc = lltm_cpp(xx[:,t,:].squeeze(), (hh, cc))    \n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "torch.cuda.synchronize()\n",
    "cpp_gpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "torch.cuda.synchronize()\n",
    "cpp_gpu_backward += time.time() - start\n",
    "\n",
    "print('lltm cpp on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cpp_gpu_forward * 1e6/input_seq_len, cpp_gpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm cuda on gpu: Forward: 100.728 us | Backward 98.394 us\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# test cuda lltm on gpu\n",
    "\n",
    "lltm_cuda = LLTMcuda(input_features, state_size).to(cuda_device)\n",
    "\n",
    "cuda_gpu_forward = 0\n",
    "cuda_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu\n",
    "hh = h_gpu\n",
    "cc = C_gpu\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):\n",
    "    new_hh, new_cc = lltm_cuda(xx[:,t,:].squeeze().contiguous(), (hh, cc))    \n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "torch.cuda.synchronize()\n",
    "cuda_gpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "torch.cuda.synchronize()\n",
    "cuda_gpu_backward += time.time() - start\n",
    "\n",
    "print('lltm cuda on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cuda_gpu_forward * 1e6/input_seq_len, cuda_gpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm fast cuda on gpu: Forward: 136.870 us | Backward 95.622 us\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# test fast cuda lltm on gpu\n",
    "\n",
    "lltm_fast = LLTMfast(input_features, state_size).to(cuda_device)\n",
    "\n",
    "fast_gpu_forward = 0\n",
    "fast_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu\n",
    "hh = h_gpu\n",
    "cc = C_gpu\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "for t in range(input_seq_len):  \n",
    "    new_hh, new_cc = lltm_fast(xx[:,t,:].squeeze().contiguous(), (hh, cc))    \n",
    "    hh = new_hh\n",
    "    cc = new_cc\n",
    "torch.cuda.synchronize()\n",
    "fast_gpu_forward += time.time() - start\n",
    "\n",
    "# backprop\n",
    "start = time.time()\n",
    "(hh.sum() + cc.sum()).backward()\n",
    "torch.cuda.synchronize()\n",
    "fast_gpu_backward += time.time() - start\n",
    "\n",
    "print('lltm fast cuda on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(fast_gpu_forward * 1e6/input_seq_len, fast_gpu_backward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm fastseq cuda on gpu: Forward: 88.870 us \n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# test fastseq cuda lltm on gpu\n",
    "\n",
    "lltm_fastseq = LLTMfastseq(input_features, state_size).to(cuda_device)\n",
    "\n",
    "fastseq_gpu_forward = 0\n",
    "fastseq_gpu_backward = 0\n",
    "\n",
    "xx = X_gpu\n",
    "hh = h_gpu\n",
    "cc = C_gpu\n",
    "\n",
    "# forward\n",
    "start = time.time()\n",
    "new_hh, new_cc = lltm_fastseq(xx, (hh, cc))    \n",
    "torch.cuda.synchronize()\n",
    "fastseq_gpu_forward += time.time() - start\n",
    "\n",
    "\n",
    "\n",
    "print('lltm fastseq cuda on gpu: Forward: {:.3f} us '.format(fastseq_gpu_forward * 1e6/input_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lltm python on cpu: Forward: 138.787 us | Backward 144.174 us\n",
      "lltm python on gpu: Forward: 145.556 us | Backward 149.473 us\n",
      "lltm cpp on cpu: Forward: 125.665 us | Backward 222.874 us\n",
      "lltm cpp on gpu: Forward: 117.089 us | Backward 316.190 us\n",
      "lltm cuda on gpu: Forward: 100.728 us | Backward 98.394 us\n",
      "lltm fast cuda on gpu: Forward: 136.870 us | Backward 95.622 us\n",
      "lltm fastseq cuda on gpu: Forward: 88.870 us \n"
     ]
    }
   ],
   "source": [
    "# print all results\n",
    "print('lltm python on cpu: Forward: {:.3f} us | Backward {:.3f} us'.format(python_cpu_forward * 1e6/input_seq_len, python_cpu_backward * 1e6/input_seq_len))\n",
    "print('lltm python on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(python_gpu_forward * 1e6/input_seq_len, python_gpu_backward * 1e6/input_seq_len))\n",
    "print('lltm cpp on cpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cpp_cpu_forward * 1e6/input_seq_len, cpp_cpu_backward * 1e6/input_seq_len))\n",
    "print('lltm cpp on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cpp_gpu_forward * 1e6/input_seq_len, cpp_gpu_backward * 1e6/input_seq_len))\n",
    "print('lltm cuda on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(cuda_gpu_forward * 1e6/input_seq_len, cuda_gpu_backward * 1e6/input_seq_len))\n",
    "print('lltm fast cuda on gpu: Forward: {:.3f} us | Backward {:.3f} us'.format(fast_gpu_forward * 1e6/input_seq_len, fast_gpu_backward * 1e6/input_seq_len))\n",
    "print('lltm fastseq cuda on gpu: Forward: {:.3f} us '.format(fastseq_gpu_forward * 1e6/input_seq_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
