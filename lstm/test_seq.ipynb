{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "from cacheLSTM.cacheLSTM import cacheLSTM\n",
    "from refLSTM import refLSTM\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "cuda_device = torch.device(\"cuda\")\n",
    "\n",
    "# init dimensions\n",
    "B = 32 # batch-size\n",
    "Di = 64 # input-features\n",
    "Dh = 32 # hidden-state size\n",
    "T = 1000 # input sequence len\n",
    "\n",
    "# init inputs\n",
    "X = torch.randn(T, B, Di, device=cuda_device)\n",
    "h = torch.randn(B, Dh, device=cuda_device)\n",
    "C = torch.randn(B, Dh, device=cuda_device)\n",
    "\n",
    "# init weights\n",
    "Wii = torch.randn(Dh, Di, device=cuda_device)\n",
    "Wif = torch.randn(Dh, Di, device=cuda_device)\n",
    "Wig = torch.randn(Dh, Di, device=cuda_device)\n",
    "Wio = torch.randn(Dh, Di, device=cuda_device)\n",
    "\n",
    "Whi = torch.randn(Dh, Dh, device=cuda_device)\n",
    "Whf = torch.randn(Dh, Dh, device=cuda_device)\n",
    "Whg = torch.randn(Dh, Dh, device=cuda_device)\n",
    "Who = torch.randn(Dh, Dh, device=cuda_device)\n",
    "\n",
    "# test each algorithm multiple iters\n",
    "test_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c-lstm cuda on gpu: Forward: 2.030 us \n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# test cache cuda lstm on gpu\n",
    "\n",
    "clstm = cacheLSTM(Di, Dh)\n",
    "clstm_uu = torch.cat([Wii.clone(), Wif.clone(), Wio.clone(), Wig.clone()], 0).transpose(0,1).contiguous()\n",
    "clstm_ww = torch.cat([Whi.clone(), Whf.clone(), Who.clone(), Whg.clone()], 0).contiguous()\n",
    "\n",
    "clstm.setUW(clstm_uu,clstm_ww)\n",
    "clstm = clstm.to(cuda_device)\n",
    "\n",
    "clstm_x = X.clone()\n",
    "clstm_h = h.clone()\n",
    "clstm_c = C.clone()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# forward\n",
    "clstm_forward = 0\n",
    "for _ in range(test_iters):\n",
    "    # prepare input\n",
    "    clstm_x = X.clone()\n",
    "    clstm_h = h.clone()\n",
    "    clstm_c = C.clone()\n",
    "    \n",
    "    # one forward pass\n",
    "    start = time.time()\n",
    "    clstm_out_h, clstm_out_c = clstm(clstm_x, clstm_h, clstm_c)    \n",
    "    torch.cuda.synchronize()\n",
    "    clstm_forward += time.time() - start\n",
    "\n",
    "clstm_forward_us = clstm_forward * 1e6 / T / test_iters\n",
    "print('c-lstm cuda on gpu: Forward: {:.3f} us '.format(clstm_forward_us))\n",
    "\n",
    "# get rid of the first in hout which is h0\n",
    "clstm_out_x = clstm_out_h[1:,:,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch cuda-optimized lstm on gpu: Forward: 7.085 us \n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# test torch lstm on gpu\n",
    "\n",
    "lstm = torch.nn.LSTM(Di, Dh, bias=False)\n",
    "lstm_uu = torch.cat([Wii.clone(), Wif.clone(), Wig.clone(), Wio.clone()], 0).contiguous()\n",
    "lstm_ww = torch.cat([Whi.clone(), Whf.clone(), Whg.clone(), Who.clone()], 0).contiguous()\n",
    "lstm.weight_ih_l0 = torch.nn.Parameter(lstm_uu)\n",
    "lstm.weight_hh_l0 = torch.nn.Parameter(lstm_ww)\n",
    "lstm = lstm.to(cuda_device)\n",
    "\n",
    "h0 = h.unsqueeze(0)\n",
    "c0 = C.unsqueeze(0)\n",
    "\n",
    "# forward\n",
    "lstm_gpu_forward = 0\n",
    "for _ in range(test_iters):\n",
    "    # prepare input\n",
    "    lstm_x = X.clone()\n",
    "    lstm_h = h0.clone()\n",
    "    lstm_c = c0.clone()\n",
    "    \n",
    "    # forward pass\n",
    "    start = time.time()\n",
    "    lstm_out_x, (lstm_out_h, lstm_out_c) = lstm(lstm_x, (lstm_h, lstm_c)) \n",
    "    torch.cuda.synchronize()\n",
    "    lstm_gpu_forward += time.time() - start\n",
    "\n",
    "lstm_gpu_forward_us = lstm_gpu_forward * 1e6/T/test_iters\n",
    "print('pytorch cuda-optimized lstm on gpu: Forward: {:.3f} us '.format(lstm_gpu_forward_us))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple ref lstm on gpu: Forward: 199.717 us \n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# test naive python lstm on gpu\n",
    "\n",
    "ref_lstm = refLSTM(B, Di, Dh, [Wii.clone(), Wif.clone(), Wig.clone(), Wio.clone(), Whi.clone(), Whf.clone(), Whg.clone(), Who.clone()])\n",
    "\n",
    "ref_lstm_gpu_forward = 0\n",
    "for _ in range(test_iters):\n",
    "    ref_X = X.clone()\n",
    "    ref_h0 = h.clone()\n",
    "    ref_c0 = C.clone()\n",
    "    \n",
    "    start = time.time()\n",
    "    reflstm_out_x, ref_cout = ref_lstm.forward(ref_X, ref_h0, ref_c0) \n",
    "    torch.cuda.synchronize()\n",
    "    ref_lstm_gpu_forward += time.time() - start\n",
    "\n",
    "ref_lstm_gpu_forward_us = ref_lstm_gpu_forward * 1e6/T/test_iters\n",
    "print('simple ref lstm on gpu: Forward: {:.3f} us '.format(ref_lstm_gpu_forward_us))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "simple ref lstm on gpu: Forward: 199.717 us \n",
      "pytorch cuda lstm on gpu: Forward: 7.085 us \n",
      "cache lstm cuda on gpu: Forward: 2.030 us \n"
     ]
    }
   ],
   "source": [
    "# check correctness\n",
    "print(torch.allclose(clstm_out_x, lstm_out_x, rtol=0, atol=1e-5))\n",
    "# print all results\n",
    "print('simple ref lstm on gpu: Forward: {:.3f} us '.format(ref_lstm_gpu_forward_us))\n",
    "print('pytorch cuda lstm on gpu: Forward: {:.3f} us '.format(lstm_gpu_forward_us))\n",
    "print('cache lstm cuda on gpu: Forward: {:.3f} us '.format(clstm_forward_us))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
